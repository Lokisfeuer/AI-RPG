{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The full code from dynamic_ai"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!pip install torchmetrics\n",
    "!pip install openai\n",
    "# importing\n",
    "import sys\n",
    "\n",
    "import numpy as np  # to handle data\n",
    "import pandas as pd  # to handle and save data\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime as d  # to generate timestamps to save models\n",
    "import math\n",
    "import random\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer  # for word embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch  # for AI\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import torch.nn.functional as F\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "import matplotlib.pyplot as plt  # to plot training\n",
    "import openai  # to generate training data\n",
    "\n",
    "import seaborn as sns  # to analyse data\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')  # uncomment this line to use the NLTK Downloader\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "class CustomTopicDataset(Dataset):  # the dataset class\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.x = sentences\n",
    "        self.y = labels\n",
    "        self.length = self.x.shape[0]\n",
    "        self.shape = self.x[0].shape[0]\n",
    "        self.feature_names = ['sentences', 'labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):  # the NN with linear relu layers and one sigmoid in the end\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack_with_sigmoid = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack_with_sigmoid(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class History:  # The history object to keep track of metrics during training and plot graphs to it.\n",
    "    def __init__(self, val_set, train_set, model, **kwargs):  # kwargs are the metrics to keep track of.\n",
    "        self.val_set = val_set\n",
    "        self.train_set = train_set\n",
    "        self.model = model\n",
    "        self.kwargs = kwargs\n",
    "        self.history = {'steps': []}\n",
    "        for i in kwargs.keys():\n",
    "            self.history.update({'val_' + i: []})\n",
    "            self.history.update({'tra_' + i: []})\n",
    "        self.valloader = None\n",
    "        self.trainloader = None\n",
    "\n",
    "    def save(self, step):  # this function is called in the training loop to save the current state of the model.\n",
    "        short_history = {}\n",
    "        for i in self.kwargs.keys():\n",
    "            short_history.update({'val_' + i: []})\n",
    "            short_history.update({'tra_' + i: []})\n",
    "        # generate two dataloader with each k entries from either the training or the validation data.\n",
    "        k = 500\n",
    "        short_train_set, waste = torch.utils.data.random_split(self.train_set, [k, len(self.train_set) - k])\n",
    "        short_val_set, waste = torch.utils.data.random_split(self.val_set, [k, len(self.val_set) - k])\n",
    "        self.valloader = DataLoader(dataset=short_val_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "        self.trainloader = DataLoader(dataset=short_train_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "        # iterate over both dataloaders simultaneously\n",
    "        for i, ((val_in, val_label), (tra_in, tra_label)) in enumerate(zip(self.valloader, self.trainloader)):\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                # predict outcomes for training and validation.\n",
    "                val_pred = self.model(val_in)\n",
    "                tra_pred = self.model(tra_in)\n",
    "                for j in self.kwargs.keys():  # iterate over the metrics\n",
    "                    # calculate metric and save to short history\n",
    "                    if len(val_pred) > 1:\n",
    "                        val_l = self.kwargs[j](val_pred, val_label).item()\n",
    "                        tra_l = self.kwargs[j](tra_pred, tra_label).item()\n",
    "                        short_history['val_' + j].append(val_l)\n",
    "                        short_history['tra_' + j].append(tra_l)\n",
    "                self.model.train()\n",
    "        # iterate over metrics and save the average of the short history to the history.\n",
    "        for i in self.kwargs.keys():\n",
    "            self.history['val_' + i].append(sum(short_history['val_' + i]) / len(short_history['val_' + i]))\n",
    "            self.history['tra_' + i].append(sum(short_history['tra_' + i]) / len(short_history['tra_' + i]))\n",
    "        self.history['steps'].append(step)  # save steps for the x-axis\n",
    "\n",
    "    # this function is called after training to generate graphs.\n",
    "    # When path is given, the graphs are saved and plt.show() is not called.\n",
    "    def plot(self, path=None):\n",
    "        figures = []\n",
    "        for i in self.kwargs.keys():  # iterate over the metrics and generate graphs for each.\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(self.history['steps'], self.history['val_' + i], 'b')\n",
    "            ax.plot(self.history['steps'], self.history['tra_' + i], 'r')\n",
    "            ax.set_title(i.upper())\n",
    "            ax.set_ylabel(i)\n",
    "            ax.set_xlabel('Epochs')\n",
    "            figures.append(fig)\n",
    "            if path is None:\n",
    "                plt.show()  # depending on the setup the graphs might still be shown even without this function called.\n",
    "            else:\n",
    "                plt.savefig(f\"{path}/{i}\")\n",
    "            plt.clf()\n",
    "        return figures\n",
    "\n",
    "\n",
    "# this function is copied from https://huggingface.co/sentence-transformers/all-roberta-large-v1\n",
    "# it returns embedded versions of the sentences its passed.\n",
    "def long_roberta(sentences):\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Sentences we want sentence embeddings for\n",
    "    # sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    # test if this works with truncation=False\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "# This function embeds the data and saves it to f'embedded_data_{topic}.pt'.\n",
    "# This function can take a while therefore it saves all embedded data every k=100 sentences in case of an error.\n",
    "def prepare_data_slowly(data, name):\n",
    "    np_data = data.to_numpy().transpose()\n",
    "    # the used model in long_roberta is SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "    embedded_data = np.array([[0, 0]])\n",
    "    k = 100\n",
    "    for i in range(math.ceil(len(np_data[0]) / k)):\n",
    "        sentences = long_roberta(list(np_data[0][k * i:k * i + k]))  # embed the sentences\n",
    "        labels = np_data[1][k * i:k * i + k]\n",
    "        a = np.array([torch.tensor_split(sentences, len(sentences)), labels])\n",
    "        a = a.transpose()\n",
    "        embedded_data = np.append(embedded_data, a, axis=0)\n",
    "        if i == 0:\n",
    "            embedded_data = embedded_data[1:]\n",
    "        torch.save(embedded_data, f'embedded_data_{name}.pt')\n",
    "        print(f'saved {i + 1} / {len(np_data[0]) / k}')\n",
    "    return embedded_data.transpose()\n",
    "\n",
    "\n",
    "# check whether the input data is short enough for word embedding.\n",
    "def check_length(data):\n",
    "    def tokenize(sentences):\n",
    "        tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "        encoded_input = tokenizer(sentences, padding=True, truncation=False, return_tensors='pt')\n",
    "        return encoded_input\n",
    "\n",
    "    sorted_data = data.reindex(data.sentences.str.len().sort_values().index[::-1]).reset_index(drop=True)\n",
    "\n",
    "    for idx, row in sorted_data.iterrows():\n",
    "        length = len(tokenize(row.sentences)['input_ids'][0])\n",
    "        if length > 512:\n",
    "            print('Warning: Paragraph longer than 512 tokens therefore it is too long and will be truncated.')\n",
    "        elif length > 128:\n",
    "            print('Warning: Paragraph longer than 128 tokens therefore longer than recommended.')\n",
    "        elif length < 80:\n",
    "            break  # as the data is sorted by (string) length there shouldn't be any problems after this point.\n",
    "\n",
    "\n",
    "# analyse the data (balance, wordcloud relative to label, input length relative to label, duplicates of inputs and\n",
    "# null inputs or labels)\n",
    "def analyse_full_data(data):\n",
    "    print('INFO')\n",
    "    data.info()\n",
    "    data.groupby(['labels']).describe()\n",
    "    print(f'Number of unique sentences: {data[\"sentences\"].nunique()}')\n",
    "    duplicates = data[data.duplicated()]\n",
    "    print(f'Number of duplicate rows:\\n{len(duplicates)}')\n",
    "    print(f'Check for nulls:\\n{data.isnull().sum()}')\n",
    "    sns.countplot(x=data['labels'])  # ploting distribution for easier understanding\n",
    "    print(data.head(3))\n",
    "\n",
    "    print('A few random examples from the dataset:')\n",
    "    # let's see how data is looklike\n",
    "    random_index = random.randint(0, data.shape[0] - 3)\n",
    "    for row in data[['sentences', 'labels']][random_index:random_index + 3].itertuples():\n",
    "        _, text, label = row\n",
    "        class_name = \"About topic\"\n",
    "        if label == 0:\n",
    "            class_name = \"Not about topic\"\n",
    "        print(f'TEXT: {text}')\n",
    "        print(f'LABEL: {label}')\n",
    "    # data contain so much garbage needs to be cleaned\n",
    "\n",
    "    truedata = data[data['labels'] == 1]\n",
    "    truedata = truedata['sentences']\n",
    "    falsedata = data[data['labels'] == 0]\n",
    "    falsedata = falsedata['sentences']\n",
    "\n",
    "    def wordcloud_draw(data, color, s):\n",
    "        words = ' '.join(data)\n",
    "        cleaned_word = \" \".join([word for word in words.split() if (word != 'movie' and word != 'film')])\n",
    "        wordcloud = WordCloud(stopwords=stopwords.words('english'), background_color=color, width=2500,\n",
    "                              height=2000).generate(cleaned_word)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.title(s)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.figure(figsize=[20, 10])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    wordcloud_draw(truedata, 'white', 'Most-common words about the topic')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    wordcloud_draw(falsedata, 'white', 'Most-common words not about the topic')\n",
    "    plt.show()  # end wordcloud\n",
    "\n",
    "    data['text_word_count'] = data['sentences'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    numerical_feature_cols = ['text_word_count']  # numerical_feature_cols = data['text_word_count']\n",
    "\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    for i, col in enumerate(numerical_feature_cols):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        sns.histplot(data=data, x=col, bins=50, color='#6495ED')\n",
    "        plt.title(f\"Distribution of Various word counts\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    for i, col in enumerate(numerical_feature_cols):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        sns.histplot(data=data, x=col, hue='labels', bins=50)\n",
    "        plt.title(f\"Distribution of Various word counts with respect to target\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class DYNAMIC_AI:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.running_loss = None\n",
    "        self.optimizer = None\n",
    "        self.dataloader = None\n",
    "        self.model = None\n",
    "        self.loss = None\n",
    "        self.dataframe = None\n",
    "        self.val_set = None\n",
    "        self.train_set = None\n",
    "        self.labels = None\n",
    "        self.sentences = None\n",
    "        self.embedded_data = None\n",
    "        self.raw_data = None\n",
    "        self.dataset = None\n",
    "\n",
    "    # generates training data (generate_data()) if real=False loads data instead\n",
    "    def generate_training_data(self, true_prompt, false_prompt, prompt_nr=100, answer_nr=100):\n",
    "        def ask_ai(prompt):  # get nr of answers from a prompt. Prompt should end with '\\n\\n1.'.\n",
    "            response = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt, temperature=1,\n",
    "                                                max_tokens=10 * answer_nr)\n",
    "            response = '1.' + response['choices'][0]['text'] + '\\n'\n",
    "            li = []\n",
    "            for i in range(answer_nr):\n",
    "                pos = response.find(str(i + 1))\n",
    "                beg = pos + len(str(i + 1)) + 2\n",
    "                end = response[beg:].find('\\n')\n",
    "                li.append(response[beg:beg + end])\n",
    "            return li\n",
    "\n",
    "        def gen_sentences(master_prompt):  # generates nr keywords to the prompt and 50*factor sentences to each\n",
    "            prompt_variations = ask_ai(master_prompt)\n",
    "            sentences = []\n",
    "            for i in prompt_variations:\n",
    "                sentences.extend(ask_ai(prompt=f'{i}\\n\\n1.'))\n",
    "            return sentences\n",
    "\n",
    "        true_master_prompt = f'Give me {prompt_nr} variations of this prompt: \"{true_prompt}\".\\n\\n1.'\n",
    "        all_sentences = gen_sentences(master_prompt=true_master_prompt)\n",
    "        false_master_prompt = f'Give me {prompt_nr} variations of this prompt: \"{false_prompt}\".\\n\\n1.'\n",
    "        all_sentences.extend(gen_sentences(master_prompt=false_master_prompt))\n",
    "        labels = []\n",
    "        for i in range(len(all_sentences)):\n",
    "            if i < len(all_sentences) / 2:\n",
    "                labels.append(True)\n",
    "            else:\n",
    "                labels.append(False)\n",
    "        data = [all_sentences, labels]\n",
    "        data = np.array(data).transpose()\n",
    "        mapping = []\n",
    "        uni = np.unique(data)\n",
    "        for i in uni:\n",
    "            mapping.append(np.where(data == i)[0][0])\n",
    "        data = data[mapping[1:]]\n",
    "        pd.DataFrame(data).to_csv(f\"{self.name.replace(' ', '_')}_generated_data.csv\", index=False,\n",
    "                                  header=['sentences', 'labels'])\n",
    "        self.raw_data = pd.read_csv(f\"{self.name.replace(' ', '_')}_generated_data.csv\")\n",
    "\n",
    "    # embeds the raw_data and creates a dataset from it\n",
    "    def embed_data(self, real=True):\n",
    "        def get_element(arr):\n",
    "            return arr[0]\n",
    "        if real:\n",
    "            self.embedded_data = prepare_data_slowly(self.raw_data, self.name)\n",
    "        else:\n",
    "            self.embedded_data = torch.load(f'embedded_data_{self.name}.pt').transpose()\n",
    "        # convert embedded data to torch dataset\n",
    "        tpl = tuple(map(get_element, tuple(np.array_split(self.embedded_data[0], len(self.embedded_data[0])))))\n",
    "        self.sentences = torch.cat(tpl)\n",
    "        self.labels = self.embedded_data[1]\n",
    "        self.labels[self.labels is True] = 1.\n",
    "        self.labels[self.labels is False] = 0.\n",
    "        self.labels = np.expand_dims(self.labels, axis=1).astype('float32')\n",
    "        self.labels = torch.from_numpy(self.labels)\n",
    "        self.dataset = CustomTopicDataset(self.sentences, self.labels)\n",
    "\n",
    "    # calls analyse_full_data with the raw_data\n",
    "    def analyse_training_data(self):\n",
    "        # check_length(self.raw_data)\n",
    "        analyse_full_data(self.raw_data)\n",
    "\n",
    "    # this function trains a model and returns it as well as the history object of its training process.\n",
    "    def train(self, epochs=10, lr=0.001, val_frac=0.1, batch_size=25, loss=nn.BCELoss()):\n",
    "        # get_acc measures the accuracy and is passed as a metric to the history object.\n",
    "        def get_acc(pred, target):\n",
    "            pred_tag = torch.round(pred)\n",
    "\n",
    "            correct_results_sum = (pred_tag == target).sum().float()\n",
    "            acc = correct_results_sum / target.shape[0]\n",
    "            acc = torch.round(acc * 100)\n",
    "\n",
    "            return acc\n",
    "\n",
    "        # generate validation dataset with the fraction of entries of the full set passed as val_frac\n",
    "        val_len = int(round(len(self.dataset) * val_frac))\n",
    "        self.train_set, self.val_set = torch.utils.data.random_split(self.dataset,\n",
    "                                                                     [len(self.dataset) - val_len, val_len])\n",
    "        self.dataloader = DataLoader(dataset=self.train_set, batch_size=batch_size, shuffle=True)\n",
    "        self.model = NeuralNetwork(self.dataset.shape)\n",
    "\n",
    "        self.loss = loss  # the loss passed to this train function\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        # define metrics to be monitored by the history object during training.\n",
    "        r2loss = R2Score()\n",
    "        mseloss = nn.MSELoss()\n",
    "        bceloss = nn.BCELoss()\n",
    "        accuracy = get_acc\n",
    "\n",
    "        history = History(self.val_set, self.train_set, self.model, r2loss=r2loss, mseloss=mseloss, accuracy=accuracy,\n",
    "                          bceloss=bceloss)  # define history object\n",
    "\n",
    "        # main training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.running_loss = 0.\n",
    "            print(f'Starting new batch {epoch + 1}/{epochs}')\n",
    "            for step, (inputs, labels) in enumerate(self.dataloader):\n",
    "                y_pred = self.model(inputs)\n",
    "                lo = self.loss(y_pred, labels)\n",
    "                lo.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.running_loss += lo.item()\n",
    "                if (step + 1) % math.floor(len(self.dataloader) / 5 + 2) == 0:  # if (step+1) % 100 == 0:\n",
    "                    print(f'current loss:\\t\\t{self.running_loss / 100}')\n",
    "                    self.running_loss = 0\n",
    "                    history.save(epoch + step / len(self.dataloader))\n",
    "                    # save current state of the model to history\n",
    "        # generate folder with timestamp and save the model there.\n",
    "        now = str(d.now().isoformat()).replace(':', 'I').replace('.', 'i').replace('-', '_')\n",
    "        os.mkdir(f\"model_{now}\")\n",
    "        torch.save(self.model, f\"model_{now}/model.pt\")\n",
    "        print(f'Model saved to \"model_{now}/model.pt\"')\n",
    "        history.plot(f\"model_{now}\")  # save graphs to the folder\n",
    "        return history, self.model  # return history and model\n",
    "\n",
    "\n",
    "# with this function you can pass custom sentences to the model\n",
    "def try_model(model):\n",
    "    a = input('Please enter your input sentence: ')\n",
    "    a = long_roberta(a)\n",
    "    pred = model(a)\n",
    "    print(pred.item())\n",
    "    print('Where 1 is the first prompt: \"\"\\nand 0 is the second: \"\".\\n')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ti = DYNAMIC_AI('topic_identifier')\n",
    "    true_prompt = 'Write a sentence about biology.'\n",
    "    false_prompt = 'Write a sentence not about biology.'\n",
    "    ti.generate_training_data(true_prompt, false_prompt, 10, 50)\n",
    "    print('ANALYSE DATA BEGINN')\n",
    "    ti.analyse_training_data()\n",
    "    print('ANALYSE DATA END')\n",
    "    ti.embed_data()\n",
    "    history, model = ti.train(epochs=10, lr=0.0001, val_frac=0.1, batch_size=10, loss=nn.BCELoss())\n",
    "    history.plot()\n",
    "    '''\n",
    "    # this part loads and tests models.\n",
    "    biology = torch.load('model_biology.pt')\n",
    "    chemistry = torch.load('model_chemistry.pt')\n",
    "    while True:\n",
    "        print('BIOLOGY:')\n",
    "        try_model(biology)\n",
    "        print('CHEMISTRY:')\n",
    "        try_model(chemistry)\n",
    "\n",
    "    # create two models, one for biology and one for chemistry\n",
    "    topic1 = 'biology'\n",
    "    topic2 = 'chemistry'\n",
    "    ti1 = DYNAMIC_AI()  # initialize topic identifiers\n",
    "    ti2 = DYNAMIC_AI()\n",
    "    ti1.generate_training_data(topic1, nr=40, fac=5)  # generate training data\n",
    "    ti2.generate_training_data(topic2, nr=40, fac=5)\n",
    "    acc1 = prompt_engineering_acc(topic1, ti1.raw_data)  # get accuracies of using openai instead\n",
    "    acc2 = prompt_engineering_acc(topic2, ti2.raw_data)\n",
    "    print(f'{topic1} prompt engineering: {acc1}')  # print those accuracies\n",
    "    print(f'{topic2} prompt engineering: {acc2}')\n",
    "    print(f'{topic1} analyse ----------------------------')  # analyse generated data\n",
    "    ti1.analyse_training_data()\n",
    "    print(f'{topic2} analyse ----------------------------')\n",
    "    ti2.analyse_training_data()\n",
    "    ti1.embed_data(topic1)  # embedd data\n",
    "    ti2.embed_data(topic2)\n",
    "    history1, model = ti1.train(epochs=10, lr=0.0001, val_frac=0.1, batch_size=10, loss=nn.BCELoss())  # train models\n",
    "    history2, model2 = ti1.train(epochs=10, lr=0.0001, val_frac=0.1, batch_size=10, loss=nn.BCELoss())\n",
    "    print(f'Now come the graphs from {topic1}.')  # plot training graphs\n",
    "    history1.plot()\n",
    "    print(f'Now come the graphs from {topic2}')\n",
    "    history2.plot()\n",
    "    '''\n",
    "\n",
    "# Far higher diversity in not topic related samples needed. Normal conversation, random sequences of letters, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
